---
layout: article
title:  "数据仓库面试步骤"
categories: DW
toc: true
image:
    teaser: /teaser/面试工作-25265315.jpg
---

> 本文主要介绍自己对于目前面试准备，面试官可能会问到的一些问题，基于个人对这部分进行梳理和总结

## 概述
俗话说：知己知彼，方能百战百胜。所以准备换工作之前，如果觉得自己不是业界的大牛级别的人物，或者好长时间没有参加过面试。我觉得还是很有必要模拟一下面试，让自己的优势发挥出来，让面试官跟着自己的节奏走。而不是让面试官想方设法的引导你，最后会将自己逼到死胡同，将自己不太擅长的方面暴露出来，影响后续的面试环节的发挥。

## 介绍
> 这个回合非常重要，如果介绍的非常好，不仅可以为后面面试官提问做伏笔，还可以让面试官对你有好感。所以建议准备一个十分钟时长的介绍。(其实也是为了暖场，避免气氛过于尴尬)

* 自我介绍（也就是基础介绍）
  我叫**，来自陕西咸阳。西安邮电大学本科毕业。毕业之后就来到北京，来14年9月想在互联网公司有所发展，来到现在的公司敦煌网。在大数据/基础架构部门担任数据开发工程师。
* 技术介绍以及项目介绍
* 技术亮点以及个人兴趣爱好介绍

## 正式技术和项目面试开始
### 项目面试

(1). __介绍目前集群的情况__：（主要是介绍下公司目前的规模以及使用的技术组件）

&emsp;&emsp;目前负责约100台服务器：Dell PowerEdge R730xd。32Core 64G内存，2TB/3TB X 6 300GB X 1

|名称  |规模 |
集群管理系统|CDH5.9.0
已使用的组件|HDFS、YARN、Flume、Spark、Oozie、Sqoop、Kafka、Hive、Hue、Zookeeper、Impala、Pig
hadoop机器数|45个
总数据量|420TB
数据日增量|2T
日任务job数|10000个
核心任务job数|4000个

(2) 项目大体介绍：
(3) 个人负责的任务：
(4) hive：
* Hive是基于Hadoop的一个数据仓库系统，在各大公司都有广泛的应用。公司数据仓库是基于Hive搭建，每天执行近万次的Hive ETL计算流程，负责每天数百GB的数据存储和分析。Hive的稳定性和性能对我们的数据分析非常关键。 

* hive的编译过程大体说下：
   * 词法分析/语法分析
       * 首先，客户端接受到请求者的sql命令，Antlr实现对sql进行词法和语法解析。将sql转化为抽象数
   * 语义分析：
       * 从Megastore获取模式信息，验证SQL语句中队表名，列名，以及数据类型的检查和隐式转换
   * 逻辑计划生成
       * 生成逻辑计划--算子树
   * 逻辑计划优化
       * 对算字数进行优化，包括列剪枝，分区剪枝，谓词下推等
   * 物理计划生成
       * 将逻辑计划生成包含由MapReduce任务组成的DAG的物理计划
   * 物理计划执行
       * 将DAG发送到Hadoop集群进行执行
* hive join的运行机制：
   * Map阶段：读取源表的数据，Map输出时候以Join on条件中的列为key，如果Join有多个关联键，则以这些关联键的组合作为key;Map输出的value为join之后所关心的(select或者where中需要用到的)列；同时在value中还会包含表的Tag信息，用于标明此value对应哪个表；按照key进行排序 
   * Shuffle阶段：根据key的值进行hash,并将key/value按照hash值推送至不同的reduce中，这样确保两个表中相同的key位于同一个reduce中
   * Reduce阶段：根据key的值完成join操作，期间通过Tag来识别不同表中的数据 
* 说说常见的hive优化方式 ：
  * 文件格式的优化： 一个是给输入和输出选择合适的文件格式，另一个则是小文件问题。小文件问题在目前的hive环境下已经得到了比较好的解决，hive的默认配置中就可以在小文件输入时自动把多个文件合并给1个map处理，输出时如果文件很小也会进行一轮单独的合并，Hive0.9版本有3种，textfile，sequencefile和rcfile。总体上来说，rcfile的压缩比例和查询时间稍好一点，所以推荐使用。
  * 并行执行Job：set hive.exec.parallel=true;   //打开任务并行执行，
  * 对hivesql检查，避免数据倾斜： 任务迚度长时间维持在99%（或100%）;查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。本地读写数据量很大。
      * 少用COUNT DISTINCT：数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换
      * 存在大量空值或NULL，或者某一个值的记录特别多，可以先把该值过滤掉，在最后单独处理
  * JVM重用：正常情况下，MapReduce启动的JVM在完成一个task之后就退出了，但是如果任务花费时间很短，又要多次启动JVM的情况下（比如对很大数据量进行计数操作），JVM的启动时间就会变成一个比较大的overhead。在这种情况下，可以使用jvm重用的参数：set mapred.job.reuse.jvm.num.tasks = 5;他的作用是让一个jvm运行多次任务之后再退出。这样一来也能节约不少JVM启动时间



 
* hive2.0新特性：
     
 hive2.0是在2016年发布的，目前应该是2.1版本，引入6大特性：

     * LLAP。Apache Hive 2.0引入了LLAP（Live Long And Process），而2.1则对其进行了极大的优化，相比于Apache Hive 1，其性能提升约25倍（它引入了分布式持久化查询服务，并结合经优化的数据缓存机制，可快速启动查询计算作业并避免无需的磁盘IO操作。简而言之，LLAP是下一代分布式计算架构，它能够智能地将数据缓存到多台机器内存中，并允许所有客户端共享这些缓存的数据，同时保留了弹性伸缩能力）
     * 更鲁邦的SQL ACID支持
     * 2X ETL性能提升。引入更智能的CBO（Cost Based Optimizer），更快的类型转换以及动态分区优化；
     * 支持存储过程。加大简化了从EDW迁移到Hive的流程。这是通过开源项目HPL/SQL（Apache开源协议，http://www.hplsql.org/）实现的，HPL/SQL的目的是为Apache Hive,SparkSQL, Impala 以及其他SQL-on-Hadoop 实现, 任何 NoSQL和 RDBMS增加存储过程的实现；
     * 对文本格式数据增加向量化计算的支持
     * 引入新的诊断和监控工具，包括新的HiveServer2 UI，LLAPUI和改进的Tez UI  

(5) spark

 * [spark完整知识体系](http://www.itdadao.com/articles/c15a1258385p0.html)
 * [spark数据倾斜优化](https://www.iteblog.com/archives/1671.html)

(6). __优化__:

* 离线计算处理：将Flume采集过来的snappy文件直接落到HDFS上，通过pig脚本解析到表，经过ods基础层-->mds中间层-->sds应用层的ETL处理，然后通过sqoop/sqlloader推送到RDBMS关系数据库中。ETL使用的hive sql/Pig计算+shell脚本来实现的。这种方式是很稳定的，就是处理时间比较长，存在效率问题。但是随着业务发展，当数据量比较大的时候，流程的运行时间较长，这些ETL流程通常处于比较上游的位置，会直接影响到一系列下游的完成时间以及各种重要数据报表的生成。更重要的是还给很多提供给卖家收费产品以及广告收费扣费产品的计算支持。
  * ETL流程优化：
     * 公司ETL处理使用的调度工具是：Crontab/rundeck
     * 使用kettle来维护ETL job依赖关系。这个开源工具相对于Oozie这种调度流程工具更界面化，易于配置管理
     * 通过对每天处理日志log进行分析，将每个任务处理时间打印出来。将依赖关系表进行调整。将表之间没有依赖关系的并行处理。充分利用集群资源。
  * hadoop集群优化
     * 升级hadoop2.0使用yarn进行资源调度，配置高可用ResourceManager HA高可用，保证集群稳定。
  * hive sql性能调优
     * 通过对源码分析。 
  * 使用spark引擎计算处理
     * 由于公司数据处理以Hive SQL为主，底层计算使用mapreduce引擎。部分相对复杂的业务会由工程师编写MapReduce程序实现。随着业务的发展，单纯的Hive SQL查询或者MapReduce程序已经越来越难以满足数据处理和分析的需求。 所以觉得应该需要引入spark计算引擎，使用hive on spark进行处理分析。大大缩短整个流程时间。
   
### 技术基础面试

### 反问面试官
&emsp;&emsp;一般面试快结束时，面试官出于礼貌会问你有什么想问我的吗？
* 我们公式大数据平台的机器规模以及人员规模的情况？
* 这个岗位具体负责哪方面？

### HR面试
HR面试主要考察一个人的价值观，潜力和职业规划。所以进入这一关之前请想清楚几个问题：

* 
* 为什么离开目前的公司？
   * 希望自己有更好的发展 
* 你的职业规划是什么？
   * 还是想走技术这条路线，未来两三年内希望自己能在数据仓库开发方面能有所突破。
* 你平时喜欢逛的开源网站有哪些？
   * stackoverflow，oschina，spark技术社区 
* 遇到的瓶颈是什么？
* 你对加班的看法？
   * 如果是工作需要的话肯定会加班，比如像我现在的公司也会遇到加班，上线新产品或者故障处理等。但是同时我会提高个人工作效率，并将一些固化工作自动化解决。减少不必要的加班。
*  你期望薪酬是多少？
   * 会先问下，公司的年终奖和绩效是怎么样的？然后给说一个工资范围.再说下我现在的公司是16个月的季度奖和年终奖。   
* 如果是BAT的话，HR还会问是否了解他们公司的价值观？
