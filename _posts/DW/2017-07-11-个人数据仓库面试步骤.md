---
layout: article
title:  "数据仓库面试步骤"
categories: DW
toc: true
image:
    teaser: /teaser/计算机数据中心-35374082.jpg
---

> 本文主要介绍自己对于目前面试准备，面试官可能会问到的一些问题，基于个人对这部分进行梳理和总结

## 概述
俗话说：知己知彼，方能百战百胜。所以准备换工作之前，如果觉得自己不是业界的大牛级别的人物，或者好长时间没有参加过面试。我觉得还是很有必要模拟一下面试，让自己的优势发挥出来，让面试官跟着自己的节奏走。而不是让面试官想方设法的引导你，最后会将自己逼到死胡同，将自己不太擅长的方面暴露出来，影响后续的面试环节的发挥。

## 介绍
> 这个回合非常重要，如果介绍的非常好，不仅可以为后面面试官提问做伏笔，还可以让面试官对你有好感。所以建议准备一个十分钟时长的介绍。(其实也是为了暖场，避免气氛过于尴尬)

* 自我介绍（也就是基础介绍）
  我叫**，来自陕西咸阳。西安邮电大学本科毕业。毕业之后就来到北京，在亚信公司做了一年的ETL工程师。后来15年9月想在互联网公司有所发展，来到现在的公司敦煌网。目前再大数据/基础架构部门担任数据开发工程师。
* 技术介绍以及项目介绍
* 技术亮点以及个人兴趣爱好介绍

## 正式技术和项目面试开始
### 项目面试
* 项目介绍
1. 介绍目前集群的情况：（主要是介绍下公司目前的规模以及使用的技术组件）

&emsp;&emsp;目前负责约100台服务器：Dell PowerEdge R730xd。32Core 64G内存，2TB/3TB X 6 300GB X 1

|名称  |规模 |
集群管理系统|CDH5.9.0
已使用的组件|HDFS、YARN、Flume、Spark、Oozie、Sqoop、Kafka、Hive、Hue、Zookeeper、Impala、Pig
hadoop机器数|45个
总数据量|420TB
数据日增量|2T
日任务job数|10000个
核心任务job数|4000个
 
2. 优化

* 离线计算处理：将Flume采集过来的snappy文件直接落到HDFS上，通过pig脚本解析到表，经过ods基础层-->mds中间层-->sds应用层的ETL处理，然后通过sqoop/sqlloader推送到RDBMS关系数据库中。ETL使用的hive sql/Pig计算+shell脚本来实现的。这种方式是很稳定的，就是处理时间比较长，存在效率问题。但是随着业务发展，当数据量比较大的时候，流程的运行时间较长，这些ETL流程通常处于比较上游的位置，会直接影响到一系列下游的完成时间以及各种重要数据报表的生成。更重要的是还给很多提供给卖家收费产品以及广告收费扣费产品的计算支持。
  * ETL流程优化：
     * 公司ETL处理使用的调度工具是：Crontab/rundeck
     * 使用kettle来维护ETL job依赖关系。这个开源工具相对于Oozie这种调度流程工具更界面化，易于配置管理
     * 通过对每天处理日志log进行分析，将每个任务处理时间打印出来。将依赖关系表进行调整。将表之间没有依赖关系的并行处理。充分利用集群资源。
  * hadoop集群优化
     * 升级hadoop2.0使用yarn进行资源调度，配置高可用ResourceManager HA高可用，保证集群稳定。
  * hive sql性能调优
     * 通过对源码分析。 
  * 使用spark引擎计算处理
     * 由于公司数据处理以Hive SQL为主，底层计算使用mapreduce引擎。部分相对复杂的业务会由工程师编写MapReduce程序实现。随着业务的发展，单纯的Hive SQL查询或者MapReduce程序已经越来越难以满足数据处理和分析的需求。 所以觉得应该需要引入spark计算引擎，使用hive on spark进行处理分析。大大缩短整个流程时间。
   
### 技术基础面试

### 反问面试官
&emsp;&emsp;一般面试快结束时，面试官出于礼貌会问你有什么想问我的吗？
* 
### HR面试
HR面试主要考察一个人的价值观，潜力和职业规划。所以进入这一关之前请想清楚几个问题：
* 为什么离开目前的公司？
* 你的职业规划是什么？
* 遇到的瓶颈是什么？
* 如果是BAT的话，HR还会问是否了解他们公司的价值观？