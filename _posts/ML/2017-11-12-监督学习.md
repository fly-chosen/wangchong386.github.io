---
layout: article
title:  "监督学习与分类回归"
categories: ML
toc: true
image:
---

> 这篇内容主要介绍监督学习的几个常见的算法思路以及应用场景。

* 主要说说监督学习：监督学习 就是分类，通过已有的训练样本去得到一个最优模型然后利用这将所有输入映射为相应的输出，对于输出进行判断实现分类这就未知数据了。监督学习中的典型例子是 KNN和 SVM。
* 再所说分类（Classification）是数据挖掘领域中的一种重要技术，它是从一组已知的训练样本中发现分类模型，并且使用这个分类模型来预测待分类样本。建立一个有效的分类算法模型最终将待分类的样本进行处理是非常有必要的。
* 分类：KNN（K-Nearest Neighbors），决策树，朴素贝叶斯分类，Logistic回归，CART分类回归树，SVM支持向量机，集成学习（Bagging,Adaboost）
## KNN算法
* KNN 算法其实简单的说就是“物以类聚”，也就是将新的没有被分类的点分类为周围的点中大多数属于的类。它采用测量不同特征值之间的距离方法进行分类，思想很简单：如果一个样本的特征空间中最为临近（欧式距离进行判断）的K个点大都属于某一个类，那么该样本就属于这个类。这就是物以类聚的思想。
* KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。
### 算法步骤：
step.1---初始化距离为最大值 初始化距离为最大值
• step.2---计算未知样本和每个训练的距离 计算未知样本和每个训练的距离 dist
• step.3---得到目前 得到目前 K个最临近样本中的大距离 maxdist
• step.4---如果 dist小于 maxdist，则将该训练样本作为 K-最近邻样本
• step.5---重复步骤 重复步骤 2、3、4，直到未知样本和所有训练的距离都算完
• step.6---统计 K-最近邻样本中每个类标号出现的次数
• step.7---选择出现频率最大的类标号 作为未知样本选择出现频率最大的类标号

### KNN优点：
* 简单，易于理解，易于实现，无需估计参数，不需要训练
* 适合对稀有事件进行分类
* 特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好
### 缺点：
1、当样本不平衡时，比如一个类的样本容量很大，其他类的样本容量很小，输入一个样本的时候，K个临近值中大多数都是大样本容量的那个类，这时可能就会导致分类错误。改进方法是对K临近点进行加权，也就是距离近的点的权值大，距离远的点权值小。

2、计算量较大，每个待分类的样本都要计算它到全部点的距离，根据距离排序才能求得K个临近点，改进方法是：先对已知样本点进行剪辑，事先去除对分类作用不大的样本。
* 适用性：
适用于样本容量比较大的类域的自动分类，而样本容量小的类域则容易误分



