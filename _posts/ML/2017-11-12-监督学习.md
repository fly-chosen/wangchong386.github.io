---
layout: article
title:  "监督学习与分类回归"
categories: ML
toc: true
image:
---

> 这篇内容主要介绍监督学习的几个常见的算法思路以及应用场景。

* 主要说说监督学习：监督学习 就是分类，通过已有的训练样本去得到一个最优模型然后利用这将所有输入映射为相应的输出，对于输出进行判断实现分类这就未知数据了。监督学习中的典型例子是 KNN和 SVM。
* 再所说分类（Classification）是数据挖掘领域中的一种重要技术，它是从一组已知的训练样本中发现分类模型，并且使用这个分类模型来预测待分类样本。建立一个有效的分类算法模型最终将待分类的样本进行处理是非常有必要的。
* 分类：KNN（K-Nearest Neighbors），决策树，朴素贝叶斯分类，Logistic回归，CART分类回归树，SVM支持向量机，集成学习（Bagging,Adaboost）
## KNN算法
* KNN 算法其实简单的说就是“物以类聚”，也就是将新的没有被分类的点分类为周围的点中大多数属于的类。它采用测量不同特征值之间的距离方法进行分类，思想很简单：如果一个样本的特征空间中最为临近（欧式距离进行判断）的K个点大都属于某一个类，那么该样本就属于这个类。这就是物以类聚的思想。
### KNN优点：
* 简单，易于理解，易于实现，无需估计参数，不需要训练
* 适合对稀有事件进行分类
* 特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好
### 缺点：
1、当样本不平衡时，比如一个类的样本容量很大，其他类的样本容量很小，输入一个样本的时候，K个临近值中大多数都是大样本容量的那个类，这时可能就会导致分类错误。改进方法是对K临近点进行加权，也就是距离近的点的权值大，距离远的点权值小。

2、计算量较大，每个待分类的样本都要计算它到全部点的距离，根据距离排序才能求得K个临近点，改进方法是：先对已知样本点进行剪辑，事先去除对分类作用不大的样本。
* 适用性：
适用于样本容量比较大的类域的自动分类，而样本容量小的类域则容易误分

